\documentclass[11pt]{article}
\usepackage{theme}
\usepackage{shortcuts}
% Document parameters
% Document title
\title{Assignment 2 (ML for TS) - MVA}
\author{
Samuel Lhayani \email{samuel.lhayani@etu.minesparis.psl.eu} \\ % student 1
Hugo Pavy \email{hugo.pavy@etu.minesparis.psl.eu} % student 2
}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{Objective.} The goal is to better understand the properties of AR and MA processes and do signal denoising with sparse coding.

\paragraph{Warning and advice.} 
\begin{itemize}
    \item Use code from the tutorials as well as from other sources. Do not code yourself well-known procedures (e.g., cross-validation or k-means); use an existing implementation. 
    \item The associated notebook contains some hints and several helper functions.
    \item Be concise. Answers are not expected to be longer than a few sentences (omitting calculations).
\end{itemize}



\paragraph{Instructions.}
\begin{itemize}
    \item Fill in your names and emails at the top of the document.
    \item Hand in your report (one per pair of students) by Sunday 7\textsuperscript{th} December 11:59 PM.
    \item Rename your report and notebook as follows:\\ \texttt{FirstnameLastname1\_FirstnameLastname1.pdf} and\\ \texttt{FirstnameLastname2\_FirstnameLastname2.ipynb}.\\
    For instance, \texttt{LaurentOudre\_ValerioGuerrini.pdf}.
    \item Upload your report (PDF file) and notebook (IPYNB file) using this link: \href{https://forms.gle/J1pdeHspSs9zNfWAA}{https://forms.gle/J1pdeHspSs9zNfWAA}.
\end{itemize}


\section{General questions}

A time series $\{y_t\}_t$ is a single realisation of a random process $\{Y_t\}_t$ defined on the probability space $(\Omega, \mathcal{F}, P)$, i.e. $y_t = Y_t(w)$ for a given $w\in\Omega$.
In classical statistics, several independent realizations are often needed to obtain a ``good'' estimate (meaning consistent) of the parameters of the process.
However, thanks to a stationarity hypothesis and a "short-memory" hypothesis, it is still possible to make ``good'' estimates.
The following question illustrates this fact.

\begin{exercise}
An estimator $\hat{\theta}_n$ is consistent if it converges in probability when the number $n$ of samples grows to $\infty$ to the true value $\theta\in\mathbb{R}$ of a parameter, i.e. $\hat{\theta}_n \xrightarrow{\mathcal{D}} \theta$.

\begin{itemize}
    \item Recall the rate of convergence of the sample mean for i.i.d.\ random variables with finite variance.
    \item Let $\{Y_t\}_{t\geq 1}$ a wide-sense stationary process such that $\sum_k |\gamma (k)| < +\infty$. 
    Show that the sample mean $\bar{Y}_n = (Y_1+\dots+Y_n)/n$ is consistent and enjoys the same rate of convergence as the i.i.d.\ case. (Hint: bound $\mathbb{E}[(\bar{Y}_n-\mu)^2]$ with the $\gamma (k)$ and recall that convergence in $L_2$ implies convergence in probability.)
\end{itemize}

\end{exercise}

\begin{solution}  % ANSWER HERE

\paragraph{Rate of convergence in the i.i.d.\ case.}
Let $(X_i)_{i\ge 1}$ be i.i.d.\ random variables with mean $\mu$ and variance $\sigma^2 < \infty$.
The sample mean is
\[
\overline{X}n = \frac{1}{n}\sum{i=1}^n X_i.
\]
We have
\[
\mathbb{E}\left[(\overline{X}_n - \mu)^2\right]
= \operatorname{Var}(\overline{X}_n)
= \frac{\sigma^2}{n}.
\]
Thus,
\[
\overline{X}_n - \mu = O_p\left(\frac{1}{\sqrt{n}}\right),
\]
so the convergence rate is \(1/\sqrt{n}\).

\paragraph{Wide-sense stationary case.}
Let $(Y_t)_{t\ge 1}$ be a wide-sense stationary process with mean $\mu$ and autocovariance function
\[
\gamma(k) = \operatorname{Cov}(Y_t, Y_{t+k}),
\]
and assume
\[
\sum_{k=-\infty}^{\infty} |\gamma(k)| < \infty.
\]

Define the sample mean
\[
\overline{Y}n = \frac{1}{n}\sum{t=1}^n Y_t.
\]

\paragraph{Variance bound.}
Using stationarity,
\[
\operatorname{Var}(\overline{Y}_n)
= \frac{1}{n^2} 
\sum_{t=1}^n \sum_{s=1}^n \gamma(t-s)
= \frac{1}{n^2}
\sum_{k=-(n-1)}^{n-1} (n - |k|)\,\gamma(k).
= \frac{1}{n}\sum_{k=-(n-1)}^{n-1} (1 - \frac{|k|}{n})\,\gamma(k)
\]
Since \(0 \le 1 - \frac{|k|}{n} \le 1\),
\[
\operatorname{Var}(\overline{Y}_n)
\le \frac{1}{n} \sum_{k=-(n-1)}^{n-1} |\gamma(k)|
\le \frac{1}{n} \sum_{k=-\infty}^{\infty} |\gamma(k)|.
\]

Let
\[
C = \sum_{k=-\infty}^{\infty} |\gamma(k)| < \infty.
\]
Then
\[
\mathbb{E}\left[(\overline{Y}_n - \mu)^2\right]
= \operatorname{Var}(\overline{Y}_n)
\le \frac{C}{n}.
\]

\paragraph{Conclusion.}
Thus,
\[
\overline{Y}_n - \mu = O_p\left(\frac{1}{\sqrt{n}}\right),
\]
which is the same rate of convergence as in the i.i.d.\ case.

Moreover, since
\[
\mathbb{E}\left[(\overline{Y}_n - \mu)^2\right] \to 0,
\]
we have \(\overline{Y}_n \to \mu\) in \(L^2\), and therefore in probability.  
Thus, \(\overline{Y}_n\) is a consistent estimator of \(\mu\).

\end{solution}


\newpage
\section{AR and MA processes}

\begin{exercise}[subtitle=Infinite order moving average MA($\infty$)]
Let $\{Y_t\}_{t\geq 0}$ be a random process defined by
\begin{equation}\label{eq:ma-inf}
    Y_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + \dots = \sum_{k=0}^{\infty} \psi_k\varepsilon_{t-k}
\end{equation}
where $(\psi_k)_{k\geq0} \subset \mathbb{R}$ ($\psi=1$) are square summable, \ie $\sum_k \psi_k^2 < \infty$ and $\{\varepsilon_t\}_t$ is a zero mean white noise of variance $\sigma_\varepsilon^2$.
(Here, the infinite sum of random variables is the limit in $L_2$ of the partial sums.)
\begin{itemize}
    \item Derive $\mathbb{E}(Y_t)$ and $\mathbb{E}(Y_t Y_{t-k})$. Is this process weakly stationary?
    \item Show that the power spectrum of $\{Y_t\}_{t}$ is $S(f) = \sigma_\varepsilon^2 |\phi(e^{-2\pi\iu f})|^2$ where $\phi(z) = \sum_j \psi_j z^j$. (Assume a sampling frequency of 1 Hz.)
\end{itemize}

The process $\{Y_t\}_{t}$ is a moving average of infinite order.
Wold's theorem states that any weakly stationary process can be written as the sum of the deterministic process and a stochastic process which has the form~\eqref{eq:ma-inf}.

\end{exercise}

\begin{solution}  % ANSWER HERE

Let $\{Y_t\}_{t\ge0}$ be defined by
\[
Y_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + \cdots
      = \sum_{k=0}^{\infty} \psi_k \varepsilon_{t-k},
\]
where $(\psi_k)_{k\ge0}\subset\mathbb{R}$ are square–summable,
$\sum_{k=0}^\infty \psi_k^2 < \infty$, and $\{\varepsilon_t\}_t$ is a zero–mean white
noise with variance $\sigma_\varepsilon^2$.

\subsection*{(a) Mean and covariance, weak stationarity}

\paragraph{Mean.}
Using linearity of expectation and $\mathbb{E}[\varepsilon_t]=0$,
\begin{align*}
\mathbb{E}[Y_t]
  &= \mathbb{E}\Bigg[ \sum_{k=0}^{\infty} \psi_k \varepsilon_{t-k} \Bigg]
   = \sum_{k=0}^{\infty} \psi_k \mathbb{E}[\varepsilon_{t-k}]
   = 0.
\end{align*}
Hence the mean is constant in $t$ and equal to $0$.

\paragraph{Autocorrelation}
For any integer lag $h$,
\[
\mathbb{E}[Y_t Y_{t-h}]
  = \mathbb{E}\!\left[ \Bigg( \sum_{k=0}^{\infty} \psi_k \varepsilon_{t-k} \Bigg)
                     \Bigg( \sum_{\ell=0}^{\infty} \psi_\ell \varepsilon_{t-h-\ell} \Bigg)
             \right].
\]
Expanding the product and using Fubini's theorem (justified by square–summability),
\[
\mathbb{E}[Y_t Y_{t-h}]
  = \sum_{k=0}^{\infty} \sum_{\ell=0}^{\infty}
      \psi_k \psi_\ell \,
      \mathbb{E}\big[ \varepsilon_{t-k} \varepsilon_{t-h-\ell} \big].
\]
Since $\{\varepsilon_t\}$ is white noise,
\[
\mathbb{E}\big[ \varepsilon_{t-k} \varepsilon_{t-h-\ell} \big]
= \begin{cases}
  \sigma_\varepsilon^2, & \text{if } t-k = t-h-\ell \\
  0, & \text{otherwise.}
  \end{cases}
\]
The condition $t-k = t-h-\ell$ is equivalent to $\ell = k-h$.
Thus only terms with $\ell = k-h$ contribute:
\[
\mathbb{E}[Y_t Y_{t-h}]
  = \sigma_\varepsilon^2 \sum_{k=0}^{\infty} \psi_k \psi_{k-h},
\]
where we adopt the convention $\psi_j = 0$ for $j<0$.
Define the autocovariance function
\[
\gamma(h) = \operatorname{Cov}(Y_t, Y_{t-h})
         = \mathbb{E}[Y_t Y_{t-h}] - \mathbb{E}[Y_t]\mathbb{E}[Y_{t-h}]
         = \sigma_\varepsilon^2 \sum_{k=0}^{\infty} \psi_k \psi_{k-h}.
\]
Because $(\psi_k)$ is square–summable, the series defining $\gamma(h)$ is absolutely
convergent and therefore finite for all $h$.

We see that $\mathbb{E}[Y_t]=0$ for all $t$ and that $\gamma(h)$ depends only on the
lag $h$, not on $t$. Hence $\{Y_t\}$ is \emph{weakly stationary}.

\subsection*{(b) Power spectrum}

The power spectrum (spectral density) of a weakly stationary process is defined by
\[
S(f) = \sum_{h=-\infty}^{\infty} \gamma(h) e^{-2\pi i f h},
\qquad f \in [-\tfrac{1}{2},\tfrac{1}{2}],
\]
for a sampling frequency of $1$ Hz.

Introduce the function
\[
\phi(z) = \sum_{k=0}^{\infty} \psi_k z^k,
\]
which is well-defined on the unit circle $|z|=1$ by square–summability of $\psi_k$.
Set $z = e^{-2\pi i f}$.
Then
\[
\phi(e^{-2\pi i f})
  = \sum_{k=0}^{\infty} \psi_k e^{-2\pi i f k}.
\]
Its complex conjugate is
\[
\overline{\phi(e^{-2\pi i f})}
  = \sum_{j=0}^{\infty} \psi_j e^{2\pi i f j}.
\]
Therefore
\begin{align*}
\bigl|\phi(e^{-2\pi i f})\bigr|^2
  &= \phi(e^{-2\pi i f}) \, \overline{\phi(e^{-2\pi i f})} \\
  &= \left( \sum_{k=0}^{\infty} \psi_k e^{-2\pi i f k} \right)
     \left( \sum_{j=0}^{\infty} \psi_j e^{2\pi i f j} \right) \\
  &= \sum_{k=0}^{\infty} \sum_{j=0}^{\infty}
        \psi_k \psi_j e^{-2\pi i f (k-j)}.
\end{align*}
Let $h = k-j$.
Then
\[
\bigl|\phi(e^{-2\pi i f})\bigr|^2
  = \sum_{h=-\infty}^{\infty}
      \left( \sum_{k=0}^{\infty} \psi_k \psi_{k-h} \right)
      e^{-2\pi i f h}.
\]
But from above,
\[
\gamma(h) = \sigma_\varepsilon^2 \sum_{k=0}^{\infty} \psi_k \psi_{k-h}.
\]
Hence
\[
\bigl|\phi(e^{-2\pi i f})\bigr|^2
  = \frac{1}{\sigma_\varepsilon^2}
    \sum_{h=-\infty}^{\infty} \gamma(h) e^{-2\pi i f h}
  = \frac{S(f)}{\sigma_\varepsilon^2}.
\]
We conclude that
\[
S(f) = \sigma_\varepsilon^2 \, \bigl|\phi(e^{-2\pi i f})\bigr|^2,
\]
as required.
\end{solution}

\newpage
\begin{exercise}[subtitle=AR(2) process]
Let $\{Y_t\}_{t\geq 1}$ be an AR(2) process, i.e.
\begin{equation}
    Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \varepsilon_t
\end{equation}
with $\phi_1, \phi_2\in\mathbb{R}$.
The associated characteristic polynomial is $\phi(z):=1-\phi_1 z - \phi_2 z^2$.
Assume that $\phi$ has two distinct roots (possibly complex) $r_1$ and $r_2$ such that $|r_i|>1$.
Properties on the roots of this polynomial drive the behavior of this process.


\begin{itemize}
    \item Express the autocovariance coefficients $\gamma(\tau)$ using the roots $r_1$ and $r_2$.
    \item Figure~\ref{fig:q-ar-2-corr} shows the correlograms of two different AR(2) processes. Can you tell which one has complex roots and which one has real roots?
    \item Express the power spectrum $S(f)$ (assume the sampling frequency is 1 Hz) using $\phi(\cdot)$.
    \item Choose $\phi_1$ and $\phi_2$ such that the characteristic polynomial has two complex conjugate roots of norm $r=1.05$ and phase $\theta=2\pi/6$. Simulate the process $\{Y_t\}_t$ (with $n=2000$) and display the signal and the periodogram (use a smooth estimator) on Figure~\ref{fig:q-ar-2}. What do you observe?
\end{itemize}


\begin{figure}
    \centering
    \begin{minipage}[t]{0.45\textwidth}
    \centerline{\includegraphics[width=\textwidth]{images/acf1.pdf}}
    \centerline{Correlogram of the first AR(2)}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}    \centerline{\includegraphics[width=\textwidth]{images/acf2.pdf}}
    \centerline{Correlogram of the second AR(2)}
    \end{minipage}
    \caption{Two AR(2) processes}\label{fig:q-ar-2-corr}
\end{figure}



\end{exercise}

\begin{solution}  % ANSWER HERE
\begin{itemize}

\item 
Let's compute $\gamma(\tau)$

\begin{align}
    \gamma(\tau) 
    &= \mathbb{E}(Y_{t+\tau}Y_t) \\
    &= \mathbb{E}((\phi_1Y_{t+\tau-1}+\phi_2Y_{t+\tau-2}+\epsilon_{t+\tau})Y_t)
\end{align}

Or since $t \neq t+\tau $, we have that $\epsilon_{t+\tau}$ is not correlated to $Y_t$

Thus, 

\begin{align}
    \gamma(\tau) 
    &= \mathbb{E}((\phi_1Y_{t+\tau-1}+\phi_2Y_{t+\tau-2})Y_t) \\
    &= \phi_1 \gamma(\tau-1) + \phi_2\gamma(\tau-2)
\end{align}

This is a second-order linear recurrence. The space of all solutions is a vector space of dimension 2. If we find two solutions that are not proportionate, all the solutions are a linear combination of them.

Let's assume that $\gamma(\tau)=r^{\tau}$. 

Thus, we have
\begin{align}
    \gamma(\tau) 
    &= \phi_1 \gamma(\tau-1) + \phi_2\gamma(\tau-2) \\
    r^{\tau} &= \phi_1r^{\tau-1} + \phi_2r^{\tau-2} \\
    r^2 &= \phi_1 r + \phi_2 \text{ (assuming that } r \neq 0)
\end{align}

Let's pose $r = \frac{1}{z}$

 We have 
 \begin{equation}
     1 - \phi_1z - \phi_2z^2=0
 \end{equation}

 Finally the autocovariance can be written as: 
 \begin{equation}
     \gamma(\tau) = \frac{A}{r_1^{\tau}} + \frac{B}{r_2^{\tau}}
 \end{equation}

If $r_1, r_2 \in \mathbb{C}$, i.e., $r_1 = re^{i\theta}$ and $r_2 = re^{-i\theta}$ (with $r > 0$ and $\theta \in \mathbb{R}$), then there exists a unique $A, B \in \mathbb{R}$ such that for all $\tau$:
    \begin{equation}
        \gamma(\tau) = \frac{1}{r^\tau} \big( A \cos(\tau \theta) + B \sin(\tau \theta) \big).
    \end{equation}
    
\item 
Knowing that, it is obvious than the first figure with the oscillations is the one with complex roots and the other one has real roots.


\item 
Let's use the lag operator $L$ such that $LY_{t}=Y_{t-1}$. The properties of $L$ give use that we can treat L "as if it was a polynomial". We can divide it, factorised by it...

We have that 
\begin{align}
    \epsilon_t 
    &= (1-\phi_1L - \phi_2L^2)Y_t \\
    &= (1-\frac{L}{r_1})(1 - \frac{L}{r_2})Y_t \\
    Y_t &= \frac{\epsilon_t}{(1-\frac{L}{r_1})(1 - \frac{L}{r_2})} \\
\end{align}

Because $|r_i|>1$, let's assume that we will always have $|\frac{L}{r_i}| < 1$, we can use the geometric serie:

\begin{align}
Y_t &= \left( \sum_{m=0}^{\infty} \frac{1}{r_1^m} L^m \right) \left( \sum_{n=0}^{\infty} \frac{1}{r_2^n} L^n \right) \epsilon_t \\
&= \left( \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} \frac{1}{r_1^m r_2^n} L^{m+n} \right) \epsilon_t \\
&= \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} \frac{1}{r_1^m r_2^n} L^{m+n} \epsilon_t \\
&= \sum_{k=0}^{\infty} \underbrace{\sum_{i+j=k} \frac{1}{r_1^i r_2^j}}_{ \theta_k } \epsilon_{t-k} \\
&= \sum_{k=0}^{\infty} \theta_k \epsilon_{t-k}
\end{align}

We finally have obtained an infinite order moving average MA($\infty$). \\

According to question 2, we know that: $S(f) = \sigma_\varepsilon^2 |\phi'(e^{-2\pi i f})|^2$. Where $\phi'(z) = \sum_j \theta_j z^j$ with $\theta_k=\sum_{i+j=k} \frac{1}{r_1^i r_2^j}$. \\
With same manipulations as before over the sum, we have $\phi = \phi'$

Finally,
\begin{equation}
    S(f) = \sigma_\varepsilon^2 |\phi(e^{-2\pi i f})|^2
\end{equation}

\end{itemize}


\begin{figure}
    \centering
    \begin{minipage}[t]{0.45\textwidth}
    \centerline{\includegraphics[width=\textwidth]{images/signal_q_3.png}}
    \centerline{Signal}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}    \centerline{\includegraphics[width=\textwidth]{images/periodogram_q_3.png}}
    \centerline{Periodogram}
    \end{minipage}
    \caption{AR(2) process}\label{fig:q-ar-2}
\end{figure}

We can observe a pick in the peridodogram at $f \approx 0.17$Hz which makes sense according to the question 3 because this is the value of $\frac{\theta}{2\pi}$ and we have complex roots.

\end{solution}

\newpage
\section{Sparse coding}

The modulated discrete cosine transform (MDCT) is a signal transformation often used in sound processing applications (for instance, to encode an MP3 file).
A MDCT atom $\phi_{L,k}$ is defined for a length 2L and a frequency localisation $k$ ($k=0,\dots,L-1$) by
\begin{equation}
\forall u=0,\dots,2L-1,\quad\phi_{L,k}[u]=w_{L}[u]\sqrt{\frac{2}{L}} \cos [ \frac{\pi}{L} \left(u+ \frac{L+1}{2}\right) (k+\frac{1}{2}) ]
\end{equation}
where $w_{L}$ is a modulating window given by
\begin{equation}
w_L[u] = \sin \left[{\frac {\pi }{2L}}\left(u+{\frac {1}{2}}\right)\right].
\end{equation}


\begin{exercise}[subtitle=Sparse coding with OMP]
For the signal provided in the notebook, learn a sparse representation with MDCT atoms.
The dictionary is defined as the concatenation of all shifted MDCDT atoms for scales $L$ in $[32, 64, 128, 256, 512, 1024]$.

\begin{itemize}
    \item For the sparse coding, implement the Orthogonal Matching Pursuit (OMP). (Use convolutions to compute the correlation coefficients.)
    \item Display the norm of the successive residuals and the reconstructed signal with 10 atoms.
\end{itemize}

\end{exercise}
\begin{solution}


\begin{figure}
    \centering
    \begin{minipage}[t]{0.45\textwidth}
    \centerline{\includegraphics[width=\textwidth]{norm_of_residual.png}}
    \centerline{Norms of the successive residuals}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}    \centerline{\includegraphics[width=\textwidth]{reconstructed_signal.png}}
    \centerline{Reconstruction with 10 atoms}
    \end{minipage}
    \caption{Question 4}
\end{figure}



\end{solution}

\end{document}
